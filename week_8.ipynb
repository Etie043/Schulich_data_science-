{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasetsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached tensorflow_datasets-4.9.7-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: immutabledict in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (4.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Requirement already satisfied: promise in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (5.28.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (5.9.8)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (18.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (2.5.0)\n",
      "Requirement already satisfied: toml in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: etils>=1.9.1 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.10.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.11.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->tensorflow_datasets) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in c:\\users\\etiem\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tensorflow-metadata->tensorflow_datasets) (1.65.0)\n",
      "Using cached tensorflow_datasets-4.9.7-py3-none-any.whl (5.3 MB)\n",
      "Installing collected packages: tensorflow_datasets\n",
      "Successfully installed tensorflow_datasets-4.9.7\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\etiem\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etiem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/2 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/3 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:   0%|          | 0/4 [00:00<?, ? url/s]\n",
      "Dl Completed...:  25%|██▌       | 1/4 [00:00<00:00,  5.55 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00, 10.21 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  6.60 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  6.21 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  5.84 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  5.45 url/s]\n",
      "Dl Completed...:  50%|█████     | 2/4 [00:00<00:00,  5.22 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  7.36 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  6.83 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  5.86 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  5.52 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  5.20 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  4.30 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.74 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.30 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:00<00:00,  3.00 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.74 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.50 url/s]\n",
      "Dl Completed...:  75%|███████▌  | 3/4 [00:01<00:00,  2.31 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.98 url/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.98 url/s]\n",
      "\u001b[A\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.98 url/s]\n",
      "Extraction completed...: 100%|██████████| 4/4 [00:01<00:00,  2.25 file/s]\n",
      "Dl Size...: 100%|██████████| 10/10 [00:01<00:00,  5.62 MiB/s]\n",
      "Dl Completed...: 100%|██████████| 4/4 [00:01<00:00,  2.25 url/s]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\etiem\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "(train, test) = tfds.load('mnist', split=['train','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_size, test_size):\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # take a sample\n",
    "    train_idx = np.random.randint(low=0, high=train[0].shape[0], size=train_size)\n",
    "    test_idx = np.random.randint(low=0, high=test[0].shape[0], size=test_size)\n",
    "    X_train = train[0][train_idx].reshape(-1,28*28)\n",
    "    y_train = train[1][train_idx].reshape(-1,1)\n",
    "    X_test = test[0][test_idx].reshape(-1,28*28)\n",
    "    y_test = test[1][test_idx].reshape(-1,1)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "    X_train = scale.fit_transform(X_train)\n",
    "    X_test = scale.transform(X_test)\n",
    "\n",
    "    OH = OneHotEncoder(categories='auto', sparse=False)\n",
    "    y_train = OH.fit_transform(y_train)\n",
    "    y_test = OH.transform(y_test)\n",
    "\n",
    "    print('X_train:',X_train.shape)\n",
    "    print('y_train:',y_train.shape)\n",
    "    print('X_test:',X_test.shape)\n",
    "    print('y_test:',y_test.shape)\n",
    "    print('Min:', X_train.min())\n",
    "    print('Max:', X_train.max())\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_model(initializer, activation='relu'):\n",
    "    return tf.keras.Sequential([\n",
    "        layers.Dense(32, activation, input_shape=(784,), kernel_initializer = initializer),\n",
    "        layers.Dense(32, activation, kernel_initializer = initializer),\n",
    "        layers.Dense(32, activation, kernel_initializer = initializer),\n",
    "        layers.Dense(32, activation, kernel_initializer = initializer),\n",
    "        layers.Dense(10, activation='softmax', kernel_initializer=tf.keras.initializers.glorot_normal())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_size, test_size):\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # take a sample\n",
    "    train_idx = np.random.randint(low=0, high=train[0].shape[0], size=train_size)\n",
    "    test_idx = np.random.randint(low=0, high=test[0].shape[0], size=test_size)\n",
    "    X_train = train[0][train_idx].reshape(-1,28*28)\n",
    "    y_train = train[1][train_idx].reshape(-1,1)\n",
    "    X_test = test[0][test_idx].reshape(-1,28*28)\n",
    "    y_test = test[1][test_idx].reshape(-1,1)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "    X_train = scale.fit_transform(X_train)\n",
    "    X_test = scale.transform(X_test)\n",
    "\n",
    "    OH = OneHotEncoder(categories='auto')\n",
    "    y_train = OH.fit_transform(y_train)\n",
    "    y_test = OH.transform(y_test)\n",
    "\n",
    "    print('X_train:',X_train.shape)\n",
    "    print('y_train:',y_train.shape)\n",
    "    print('X_test:',X_test.shape)\n",
    "    print('y_test:',y_test.shape)\n",
    "    print('Min:', X_train.min())\n",
    "    print('Max:', X_train.max())\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 784)\n",
      "y_train: (60000, 10)\n",
      "X_test: (10000, 784)\n",
      "y_test: (10000, 10)\n",
      "Min: -1.2795504110416283\n",
      "Max: 244.94693302875552\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = load_dataset(60000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etiem\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert elements of SparseTensor(indices=Tensor(\"data_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"data_2:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"data_3:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model_zeros \u001b[38;5;241m=\u001b[39m simple_model(init, activate)\n\u001b[0;32m      5\u001b[0m model_zeros\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel_zeros\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py:570\u001b[0m, in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcategorical_crossentropy\u001b[39m(target, output, from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    532\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Categorical crossentropy between an output tensor and a target tensor.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \n\u001b[0;32m    534\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m    [0. 0. 0.]\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    571\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(output)\n\u001b[0;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Failed to convert elements of SparseTensor(indices=Tensor(\"data_1:0\", shape=(None, 2), dtype=int64), values=Tensor(\"data_2:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"data_3:0\", shape=(2,), dtype=int64)) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes."
     ]
    }
   ],
   "source": [
    "init = tf.initializers.zeros()\n",
    "activate = 'relu'\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "model_zeros = simple_model(init, activate)\n",
    "model_zeros.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_zeros.fit(X_train, y_train, epochs=10, batch_size=3200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 784)\n",
      "y_train: (60000, 10)\n",
      "X_test: (10000, 784)\n",
      "y_test: (10000, 10)\n",
      "Min: -1.2690538495049195\n",
      "Max: 244.94693302860162\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'simple_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m activate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m model_zeros \u001b[38;5;241m=\u001b[39m \u001b[43msimple_model\u001b[49m(init, activate)\n\u001b[0;32m     33\u001b[0m model_zeros\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     34\u001b[0m model_zeros\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3200\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'simple_model' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_size, test_size):\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # take a sample\n",
    "    train_idx = np.random.randint(low=0, high=train[0].shape[0], size=train_size)\n",
    "    test_idx = np.random.randint(low=0, high=test[0].shape[0], size=test_size)\n",
    "    X_train = train[0][train_idx].reshape(-1,28*28)\n",
    "    y_train = train[1][train_idx].reshape(-1,1)\n",
    "    X_test = test[0][test_idx].reshape(-1,28*28)\n",
    "    y_test = test[1][test_idx].reshape(-1,1)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "    X_train = scale.fit_transform(X_train)\n",
    "    X_test = scale.transform(X_test)\n",
    "\n",
    "    OH = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "    y_train = OH.fit_transform(y_train)\n",
    "    y_test = OH.transform(y_test)\n",
    "\n",
    "    print('X_train:',X_train.shape)\n",
    "    print('y_train:',y_train.shape)\n",
    "    print('X_test:',X_test.shape)\n",
    "    print('y_test:',y_test.shape)\n",
    "    print('Min:', X_train.min())\n",
    "    print('Max:', X_train.max())\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 784)\n",
      "y_train: (60000, 10)\n",
      "X_test: (10000, 784)\n",
      "y_test: (10000, 10)\n",
      "Min: -1.2778206931067648\n",
      "Max: 244.94693302867978\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'simple_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m activate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     31\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m model_zeros \u001b[38;5;241m=\u001b[39m \u001b[43msimple_model\u001b[49m(init, activate)\n\u001b[0;32m     33\u001b[0m model_zeros\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     34\u001b[0m model_zeros\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3200\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'simple_model' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_size, test_size):\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "    # take a sample\n",
    "    train_idx = np.random.randint(low=0, high=train[0].shape[0], size=train_size)\n",
    "    test_idx = np.random.randint(low=0, high=test[0].shape[0], size=test_size)\n",
    "    X_train = train[0][train_idx].reshape(-1,28*28)\n",
    "    y_train = train[1][train_idx].reshape(-1,1)\n",
    "    X_test = test[0][test_idx].reshape(-1,28*28)\n",
    "    y_test = test[1][test_idx].reshape(-1,1)\n",
    "\n",
    "    scale = StandardScaler()\n",
    "    X_train = scale.fit_transform(X_train)\n",
    "    X_test = scale.transform(X_test)\n",
    "\n",
    "    OH = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "    y_train = OH.fit_transform(y_train)\n",
    "    y_test = OH.transform(y_test)\n",
    "\n",
    "    print('X_train:',X_train.shape)\n",
    "    print('y_train:',y_train.shape)\n",
    "    print('X_test:',X_test.shape)\n",
    "    print('y_test:',y_test.shape)\n",
    "    print('Min:', X_train.min())\n",
    "    print('Max:', X_train.max())\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = load_dataset(60000, 10000)\n",
    "init = tf.initializers.zeros()\n",
    "activate = 'relu'\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "model_zeros = simple_model(init, activate)\n",
    "model_zeros.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_zeros.fit(X_train, y_train, epochs=10, batch_size=3200, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initializers.random_normal()\n",
    "activate = 'relu'\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "model_normal = simple_model(init, activate)\n",
    "model_normal.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_normal.fit(X_train, y_train, epochs=10, batch_size=3200, validation_data=(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
